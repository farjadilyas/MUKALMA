{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd842fad",
   "metadata": {},
   "source": [
    "# Document Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a61cc",
   "metadata": {},
   "source": [
    "A notebook for offline document selection from the cached articles. This extends the functionality in the current KnowledgeSource.py class and adds offline methods for fetching the top 3 most relevant article from the DB in the given cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dc5fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e7d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import for scraping data off the web\n",
    "import requests\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Import SentenceTransformer for using a Sentence Embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Imports for Document Similarity computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For dividing lists into smaller chunks\n",
    "from itertools import islice\n",
    "\n",
    "# For saving the KnowledgeSource object, so next time the cache can be loaded in a hot state\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "# Importing the SentenceModel\n",
    "from SentenceModel import SentenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51934a79",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e87c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_KS_FILENAME = 'knowledge_source.pkl'\n",
    "MULTI_KS_FILENAME = 'multi_knowledge_source.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dc1af",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29b7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_db(db1_filename, db2_filename, target_filename):\n",
    "    target_db = None\n",
    "    db1 = read_object(db1_filename)\n",
    "    db2 = read_object(db2_filename)\n",
    "\n",
    "    if db1 is None and db2 is not None:\n",
    "        target_db = db2\n",
    "    elif db1 is not None and db2 is None:\n",
    "        target_db = db1\n",
    "    elif db1 is not None and db2 is not None:\n",
    "        target_db = db1.copy()\n",
    "        target_db.update(db2)\n",
    "\n",
    "    if target_db is not None:\n",
    "        save_object(target_db, target_filename)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def read_object(filename):\n",
    "    obj = None\n",
    "    try:\n",
    "        if exists(filename):\n",
    "            print(f\"{filename} exists\")\n",
    "            with open(filename, 'rb') as inp:\n",
    "                obj = pickle.load(inp)\n",
    "    except (FileNotFoundError, PermissionError):\n",
    "        pass\n",
    "    return obj\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d8651",
   "metadata": {},
   "source": [
    "### Extracting paragraphs and heads from the given wikipedia article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c563b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paras_and_heads(doc_title, chunk_size=8):\n",
    "    \"\"\"\n",
    "      Returns the (heading, paragraphs) pairs from the page parsed by soup\n",
    "      Placed a minimum number of sentence limit on paragraph length to ignore insignificant paragraphs\n",
    "      which skew the results\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch and scrape the contents of the wikipedia page corresponding to the title\n",
    "    page = requests.get(f\"https://en.wikipedia.org/wiki/{doc_title}\")\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    # Extract paragraph text, ignoring any empty class paragraphs\n",
    "    # Fetch the intro paragraph separately since it isn't associated with a heading\n",
    "    # Handles the case of multiple paragraphs under a single ehading\n",
    "    paras = []\n",
    "    all_paragraphs = soup.find_all('p', class_=lambda x: x != 'mw-empty-elt')\n",
    "    intro_para = \"\"\n",
    "\n",
    "    for p_id, paragraph in enumerate(all_paragraphs):\n",
    "        p_text = re.sub(r\"\\[.*?\\]+\", '', paragraph.text)\n",
    "        p_tok = nltk.tokenize.sent_tokenize(p_text)\n",
    "        if p_id == 0:\n",
    "            intro_para = p_text\n",
    "        elif len(p_tok) > 1:\n",
    "            paras.extend([' '.join(p_chunk) for p_chunk in chunk(p_tok, chunk_size)])\n",
    "\n",
    "    # Extract text from paragraph headers\n",
    "    heads = []\n",
    "    for head in soup.find_all('span', attrs={'mw-headline'}):\n",
    "        heads.append(str(head.text))\n",
    "\n",
    "    if len(paras) == 0:\n",
    "        return None\n",
    "\n",
    "    # The first paragraph is the introductory paragraph and doesn't have a heading\n",
    "    # Set its heading as the document title\n",
    "    heads.insert(0, doc_title)\n",
    "    paras.insert(0, intro_para)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len(paras)):\n",
    "        if len(nltk.tokenize.sent_tokenize(paras[i])) > 1:\n",
    "            print(paras[i], \"\\n\")\n",
    "    \"\"\"\n",
    "    return heads, paras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad345d34",
   "metadata": {},
   "source": [
    "### TF-IDF Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a31f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_similarity(base_document, documents):\n",
    "    # To make uniformed vectors, both documents need to be combined first.\n",
    "    d = [base_document]\n",
    "    d.extend(documents)\n",
    "\n",
    "    # TODO: Hyper-parameter tuning of TF-IDF vectorizer for calculating document embeddings\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', binary=True, ngram_range=(1, 3), analyzer='char', lowercase=True)\n",
    "    embeddings = vectorizer.fit_transform(d)\n",
    "    print(embeddings.shape, len(d))\n",
    "\n",
    "    cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
    "    print(f\"DOCUMENT COSINE SIM: {cosine_similarities}\")\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ac1b4",
   "metadata": {},
   "source": [
    "## KnowledgeSource class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1142a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeSource:\n",
    "    def __init__(self, model=None, num_results=3, persist=True, persist_path='', use_hot_cache=True, offline=False):\n",
    "        self.use_multi_source = num_results > 1\n",
    "        if self.use_multi_source:\n",
    "            self.ks_filename = MULTI_KS_FILENAME\n",
    "        else:\n",
    "            self.ks_filename = SINGLE_KS_FILENAME\n",
    "\n",
    "        # Initialize knowledge source, optionally from a previously persisted file\n",
    "        # For each cached article, this dictionary stores the article's overall content, its mini paragraphs, and the\n",
    "        # precomputed embeddings corresponding to the mini paragraphs\n",
    "        persist_location = persist_path + \\\n",
    "                           ('/' if persist_path != '' and persist_path[-1] != '/' else '') + self.ks_filename\n",
    "        ks = read_object(persist_location) if use_hot_cache else None\n",
    "        if ks is None:\n",
    "            print(\"CREATING NEW KNOWLEDGE SOURCE\")\n",
    "            self.article_db = {}\n",
    "        else:\n",
    "            print(\"USING PERSISTED KNOWLEDGE SOURCE\")\n",
    "            self.article_db = ks\n",
    "\n",
    "        self.num_results = num_results\n",
    "\n",
    "        self.sentence_model = SentenceTransformer('../../../models/all-MiniLM-L6-v2', device='cuda') if model is None \\\n",
    "            else model\n",
    "\n",
    "        # Whether the knowledge source object should be persisted at the end of execution\n",
    "        self.persist = persist\n",
    "        self.persist_location = persist_location if persist else None\n",
    "        \n",
    "        # Whether to use Wikipedia or to use cached database\n",
    "        self.offline = offline\n",
    "\n",
    "    def build_db(self, topics):\n",
    "        \"\"\"\n",
    "          Given a list of topics, each consisting of a list of keywords corresponding to the topic, build a database of\n",
    "          articles consisting of an num_articles articles for each topic in the list. Stores the overall content,\n",
    "          breaks down the content of the article into mini docs, and precomputes embeddings corresponding to these mini\n",
    "          docs.\n",
    "        \"\"\"\n",
    "        for topic_keywords in topics:\n",
    "            topic_search_str = ' '.join(topic_keywords)\n",
    "\n",
    "            print(f\"Fetching data for {topic_search_str}\")\n",
    "            articles = wikipedia.search(topic_search_str, results=self.num_results)\n",
    "            print(f\"Using the following relevant articles: {articles}\")\n",
    "\n",
    "            # For every article corresponding to the topic, fetch and save its overall content and embeddings\n",
    "            # corresponding to the mini paragraphs of the article\n",
    "            for article in articles:\n",
    "                if self.use_multi_source:\n",
    "                    self.__fetch_multi_source_article_data(article)\n",
    "                else:\n",
    "                    self.__fetch_single_source_article_data(article)\n",
    "\n",
    "    def fetch_relevant_articles(self, topics):\n",
    "        \"\"\"\n",
    "            Fetches the top {self.num_results} topics from either the wikipedia or \n",
    "            local database depending on the mode. Topics is a list of keywords to search from\n",
    "        \"\"\"\n",
    "        if len(topics) == 0:\n",
    "            return []\n",
    "\n",
    "        articles = []\n",
    "        \n",
    "        # If we are not offline we search the keywords in Wikipedia\n",
    "        if not self.offline:\n",
    "            topic_search_str = ' '.join(topics)\n",
    "\n",
    "            print(f\"Fetching data for '{topic_search_str}'\")\n",
    "            try:\n",
    "                articles = wikipedia.search(topic_search_str, results=self.num_results)\n",
    "            except:\n",
    "                articles = []\n",
    "            print(f\"Using the following relevant articles: {articles}\")\n",
    "            \n",
    "        else:\n",
    "            # Searching from the local database\n",
    "            stored_articles = list(self.article_db.keys())\n",
    "            topic_search_str = ' '.join(topics)\n",
    "            all_articles = []\n",
    "            \n",
    "            for article in stored_articles:\n",
    "                all_articles.append(self.article_db[article][0])\n",
    "                \n",
    "            doc_c_sim = calculate_tfidf_similarity(topic_search_str, all_articles)    \n",
    "            selected_article_ids = doc_c_sim.argsort()\n",
    "            \n",
    "            added = 0\n",
    "            for _id in np.flip(selected_article_ids):\n",
    "                articles.append(stored_articles[_id])\n",
    "                added += 1\n",
    "                if added == self.num_results:\n",
    "                    break\n",
    "            # End for\n",
    "        # End else\n",
    "\n",
    "        return articles\n",
    "    \n",
    "    # End of function\n",
    "\n",
    "    def fetch_topic_data(self, topics, message):\n",
    "        selected_para_tok, selected_article_title = self.__fetch_multi_source_topic_data(topics, message) \\\n",
    "            if self.use_multi_source else self.__fetch_single_source_topic_data(topics, message)\n",
    "        print(f\"======= SELECTED PARAGRAPH:\\n{selected_para_tok}\")\n",
    "        return selected_para_tok, selected_article_title\n",
    "\n",
    "    def __fetch_multi_source_topic_data(self, topics, message):\n",
    "        articles = self.fetch_relevant_articles(topics)\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        topic_paras = []    # Mini paragraphs comprising all mini paragraphs of each article relevant to the topic\n",
    "        for article in articles:\n",
    "            article_paras = self.__fetch_multi_source_article_data(article)\n",
    "\n",
    "            if article_paras is not None:\n",
    "                topic_paras.extend(article_paras)\n",
    "\n",
    "        if len(topic_paras) == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        # Once the topic paras have been accumulated, the embeddings for them, and the most similar para can be found\n",
    "        topic_paras_embeddings = self.sentence_model.encode(topic_paras)\n",
    "\n",
    "        # Encode the input message, and use the topic para embeddings to calculate cosine similarity\n",
    "        # The resulting scores can be used to find the most similar paragraph in all articles relevant to the topic\n",
    "        para_c_sim = cosine_similarity(self.sentence_model.encode([message]), topic_paras_embeddings).flatten()\n",
    "        selected_para_id = para_c_sim.argmax()\n",
    "\n",
    "        return nltk.tokenize.sent_tokenize(topic_paras[selected_para_id]), articles[0]\n",
    "\n",
    "    def __fetch_multi_source_article_data(self, title):\n",
    "        article_paras = self.article_db.get(title, None)\n",
    "        if article_paras is not None:\n",
    "            print(f\"Topic '{title}' has already been cached\")\n",
    "        else:\n",
    "            print(f\"Topic '{title}' has not been cached, fetching and building...\")\n",
    "            article_data = extract_paras_and_heads(title, chunk_size=3)\n",
    "\n",
    "            if article_data is not None:\n",
    "                heads, article_paras = article_data\n",
    "                # Obtained mini docs from the article data, and filter out paragraphs that are too short\n",
    "                processed_paras = []\n",
    "                for i in range(len(article_paras)):\n",
    "                    para_tok = nltk.tokenize.sent_tokenize(article_paras[i])\n",
    "                    if len(para_tok) <= 1:\n",
    "                        continue\n",
    "                    processed_paras.append(article_paras[i])\n",
    "                article_paras = processed_paras\n",
    "\n",
    "                # Cache article paras\n",
    "                self.article_db[title] = article_paras\n",
    "            else:\n",
    "                print(f\"Data for '{title}' could not be parsed, ignoring this article\")\n",
    "\n",
    "        return article_paras\n",
    "\n",
    "    def __fetch_single_source_topic_data(self, topics, message):\n",
    "        articles = self.fetch_relevant_articles(topics)\n",
    "\n",
    "        if articles is None or len(articles) == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        docs_content = []\n",
    "        articles_data = []\n",
    "        for article in articles:\n",
    "            article_data = self.__fetch_single_source_article_data(article)\n",
    "\n",
    "            # Ignore article if its data couldn't be scraped\n",
    "            if article_data is None:\n",
    "                continue\n",
    "\n",
    "            docs_content.append(article_data[0])\n",
    "            articles_data.append(article_data)\n",
    "\n",
    "        # Obtain the cosine similarity of the message with the relevant articles for the current topic\n",
    "        if len(docs_content) == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        doc_c_sim = calculate_tfidf_similarity(message, docs_content)\n",
    "\n",
    "        # Select the most similar article\n",
    "        selected_article_id = doc_c_sim.argmax()\n",
    "        print(f\"'{articles[selected_article_id]}' has been selected as the most relevant article\")\n",
    "\n",
    "        # Now, select the most relevant mini doc (chunk of one or more paragraphs) in the most relevant article\n",
    "        selected_article_data = articles_data[selected_article_id]\n",
    "        mini_doc_embeddings, mini_docs = selected_article_data[1], selected_article_data[2]\n",
    "\n",
    "        # Calculate embedding for the input message using the saved vectorizer for this article\n",
    "        # Use this with the pre-calculated embeddings for the paragraphs in the article to calculate cosine similarity\n",
    "        para_c_sim = cosine_similarity(self.sentence_model.encode([message]), mini_doc_embeddings).flatten()\n",
    "        selected_para_id = para_c_sim.argmax()\n",
    "\n",
    "        return mini_docs[selected_para_id], articles[selected_article_id]\n",
    "\n",
    "    def __fetch_single_source_article_data(self, title):\n",
    "        article_db_entry = self.article_db.get(title, None)\n",
    "        if article_db_entry is not None:\n",
    "            print(f\"Topic '{title}' has already been cached\")\n",
    "            return article_db_entry\n",
    "\n",
    "        print(f\"Topic '{title}' has not been cached, fetching and building...\")\n",
    "\n",
    "        article_data = extract_paras_and_heads(title, chunk_size=3)\n",
    "\n",
    "        # If no paragraphs could be retrieved, this article is useless\n",
    "        if article_data is None:\n",
    "            return None\n",
    "\n",
    "        heads, paras = article_data\n",
    "\n",
    "        # Use the heads, paras pairs (mini-documents) to calculate TF-IDF embeddings...\n",
    "        # for the set of mini-documents of the article corresponding to title\n",
    "        # These embeddings can be used to find which mini_doc is most similar to a given document\n",
    "        # the absence of the heading in a sentence doesn't make it irrelevant\n",
    "\n",
    "        # Save tokenized paragraph sentences for QA, and process the paragraph text for calculating embeddings\n",
    "        content = \"\"\n",
    "        mini_docs = []\n",
    "        for i in range(len(paras)):\n",
    "            para_tok = nltk.tokenize.sent_tokenize(paras[i])\n",
    "            if len(para_tok) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Accumulate the total content of the article\n",
    "            content = f\"{content} {paras[i]}\"\n",
    "\n",
    "            # Save the tokenized sentences for this paragraph, NOTE: these use the original sentences\n",
    "            # If this paragraph gets selected as the most relevant one...\n",
    "            # Then this list of tokenized sentences will be used for question answering\n",
    "            mini_docs.append(para_tok)\n",
    "\n",
    "        # Calculate mini_doc embeddings using the processed paragraphs\n",
    "        mini_doc_embeddings = self.sentence_model.encode(paras)\n",
    "\n",
    "        # For this new article, save the following:\n",
    "        # entire article text content: used to calculate document-level embeddings when selecting most relevant document\n",
    "        # embeddings of the mini docs of this article: used when selecting the most-relevant paragraph\n",
    "        # the tokenized sentences of the mini docs of this article: uses when applying QA to the most relevant paragraph\n",
    "        article_db_entry = (content, mini_doc_embeddings, mini_docs)\n",
    "        self.article_db[title] = article_db_entry\n",
    "\n",
    "        return article_db_entry\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "         Save the knowledge acquired over the course of Knowledge Source's lifetime to disk\n",
    "        \"\"\"\n",
    "        if not self.persist:\n",
    "            return\n",
    "        save_object(self.article_db, self.persist_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff0e0c",
   "metadata": {},
   "source": [
    "## Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034934a",
   "metadata": {},
   "source": [
    "#### Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f528c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06:01:41] </span><span style=\"font-weight: bold\">[</span>SentenceModel<span style=\"font-weight: bold\">]</span>: Loading ..<span style=\"color: #800080; text-decoration-color: #800080\">/../models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">all-mpnet-base-v2...</span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">SentenceModel.py:45</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06:01:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSentenceModel\u001b[1m]\u001b[0m: Loading ..\u001b[35m/../models/\u001b[0m\u001b[95mall-mpnet-base-v2...\u001b[0m     \u001b[2mSentenceModel.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m45\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                               \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../res/knowledge_presets/multi_knowledge_source.pkl exists\n",
      "USING PERSISTED KNOWLEDGE SOURCE\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceModel('../../models/all-mpnet-base-v2', use_cuda=False)\n",
    "knowledge_db = KnowledgeSource(model=sentence_model.model, num_results=3, persist=True,\n",
    "                                            persist_path='../../res/knowledge_presets', use_hot_cache=True, offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee0029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 3092) 18\n",
      "DOCUMENT COSINE SIM: [0.18987818 0.1425059  0.13238943 0.11496557 0.10501283 0.09158884\n",
      " 0.04689385 0.0574866  0.07880864 0.0667633  0.04302766 0.04029826\n",
      " 0.04946291 0.04740884 0.05135179 0.06128076 0.05677339]\n",
      "Topic 'Pfizer–BioNTech COVID-19 vaccine' has already been cached\n",
      "Topic 'Pfizer' has already been cached\n",
      "Topic 'COVID-19 vaccination in Bangladesh' has already been cached\n",
      "======= SELECTED PARAGRAPH:\n",
      "[\"On November 9, 2020, Pfizer announced that BioNTech's COVID-19 vaccine, tested on 43,500 people, was found to be 90% effective at preventing symptomatic COVID-19.\", 'The efficacy was updated to 95% a week later.', 'Akiko Iwasaki, an immunologist interviewed by the New York Times, described the efficacy figure as \"really a spectacular number.\"']\n"
     ]
    }
   ],
   "source": [
    "knowledge_text, knowledge_article = knowledge_db.fetch_topic_data(\n",
    "    ['vaccine', 'Pfizer', 'one'], \n",
    "    \"I heard Pfizer's vaccine was one that was particularly effective\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cc6db",
   "metadata": {},
   "source": [
    "### Printing out all the stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a9db30b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pfizer–BioNTech COVID-19 vaccine\n",
      "Pfizer\n",
      "COVID-19 vaccination in Bangladesh\n",
      "Moderna COVID-19 vaccine\n",
      "Moderna\n",
      "MRNA vaccine\n",
      "COVID-19 pandemic in the United States\n",
      "COVID-19 misinformation\n",
      "John F. Kennedy assassination conspiracy theories\n",
      "Conspiracy theories related to the Trump–Ukraine scandal\n",
      "Spygate (conspiracy theory)\n",
      "George Soros\n",
      "Mandatory Fun\n",
      "TikTok\n",
      "Conspiracy Theories and Interior Design\n",
      "Toyota in Formula One\n",
      "The Lunar Injection Kool Aid Eclipse Conspiracy\n"
     ]
    }
   ],
   "source": [
    "for key in knowledge_db.article_db.keys():\n",
    "    print (key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
