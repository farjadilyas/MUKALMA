{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a967cd8-394c-4f41-806b-2bbf52c418ab",
   "metadata": {},
   "source": [
    "# Extracting Relevent Data from Wikipedia\n",
    "This notebook aims to extract the entire content of a set of relevant Wikipedia articles given a sentence / topic tap, some hook for relevant information. This will go into the first layer of our knowledge retrieval processing layer which will select the most relevant document, then the second layer will be responsible for finding the most similar sentence.\n",
    "\n",
    "Document here can refer to any granularity. At the moment, I am considering it should be at the level of paragraphs.\n",
    "\n",
    "TO BE DECIDED: Our approach for the knowledge retrieval processing layers. Which layer should we allocate more resources to?\n",
    "\n",
    "First Layer: More likely to find the paragraph which is, in fact, most relevant, but may compromise on selecting the most relevant sentence form that paragraph\n",
    "\n",
    "Second Layer: Opposite tradeoff to the first layer\n",
    "\n",
    "My take: I suspect a crude method like TF-IDF may be accurate enough to get us close enough accuracy compared to NN-based models for the first layer, after which we can use a more sophisticated model using sentence embeddings for sentence similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff02d7-14e7-4eb8-98df-141214654087",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0a226b-7655-4aae-992b-f7e5547c42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64867a66-de45-4d0d-a4b7-80b1375e5d22",
   "metadata": {},
   "source": [
    "## Simulate extracting pos from message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb19410-9b0d-4f82-9b07-51730a361186",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['cricket', 'Pakistan', 'vs', 'Australia']\n",
    "search_str = ' '.join(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be8cba-1daa-4e1e-8af4-e4f5b990d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"Austalian\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"American\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9fba8-42ff-4233-8599-7275da6e6587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f681b0-51db-41d7-b65e-e89e6a4cb6b7",
   "metadata": {},
   "source": [
    "## Test: Use the python wikipedia library to search for relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7297a689-0563-480d-98a1-7afb9c8adf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010–11 FC Barcelona season',\n",
       " 'Marc Overmars',\n",
       " 'Royston Drenthe',\n",
       " 'José Mourinho']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(\"game Barcelona match yesterday day football\", results = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d46251-08ea-4721-9d6d-4308d2b6ffa3",
   "metadata": {},
   "source": [
    "## Use BeautifulSoup to obtain paragraph data from Wikipedia Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5758f-a5eb-4996-bf36-f6a5cdd4b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_retrieve_from_wikipedia(search_item, num_results):\n",
    "    print(f\"Fetching data for {search_item}\")\n",
    "    articles = wikipedia.search(search_item, results = num_results)\n",
    "    print(f\"Using the following relevant articles: {articles}\")\n",
    "\n",
    "    documents = []\n",
    "    document = \"\"\n",
    "    for article in articles:\n",
    "        page = requests.get(f\"https://en.wikipedia.org/wiki/{article}\")\n",
    "\n",
    "        # scrape webpage\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # find and save all occurences of the paragraph tag <p> in HTML\n",
    "        p_tags = soup.find_all('p')\n",
    "\n",
    "        document = \"\"\n",
    "        for p_tag in p_tags:\n",
    "            p_text = p_tag.get_text()\n",
    "            document = f\"{document} {p_text}\"\n",
    "        \n",
    "        # Print a preview of the article\n",
    "        print(f\"{article} article preview: {p_tags[0].get_text()}...\")\n",
    "        \n",
    "        # Add current compiled document to the list of documents corresponding to articles\n",
    "        documents.append(document)\n",
    "    return articles, documents\n",
    "        \n",
    "articles, documents = search_and_retrieve_from_wikipedia(search_str, 4)\n",
    "turn = \"I've been good! Just watching some cricket. Have you been watching Pakistan vs Australia?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e82c1b-29d3-4788-bb66-80ac0e06b769",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "Plugging in Document Similarity code here since it fits in with data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fa16d-92b5-4d5b-8032-21197a834f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "def process_tfidf_similarity(base_document, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # To make uniformed vectors, both documents need to be combined first.\n",
    "    d = [base_document]\n",
    "    d.extend(documents)\n",
    "    embeddings = vectorizer.fit_transform(documents)\n",
    "    embeddings = vstack((vectorizer.transform([base_document]), embeddings))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    eds = vectorizer.fit_transform(d)\n",
    "    print(type(embeddings))\n",
    "    print(embeddings.shape, len(d))\n",
    "\n",
    "    cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    \n",
    "    cosine_similarities = cosine_similarity(eds[0:1], eds[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab33a7a-22ca-4044-abb3-7e5e38cc50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Check overlap between article title and the pos list, if noticeable, then pick that one without checking similarity\n",
    "best_articles = []\n",
    "max_overlap = -1\n",
    "for article in articles:\n",
    "    title_overlap_count = 0\n",
    "    w_list = article.split(' ')\n",
    "    for w in w_list:\n",
    "        if w in pos:\n",
    "            title_overlap_count += 1\n",
    "    \n",
    "    print(f\"overlap for '{article}' is: {title_overlap_count}\")\n",
    "    if title_overlap_count > max_overlap:\n",
    "        best_articles = [article]\n",
    "        max_overlap = title_overlap_count\n",
    "    elif title_overlap_count == max_overlap:\n",
    "        best_articles.append(article)\n",
    "\"\"\"\n",
    "#if len(best_articles) > 1:\n",
    "documents = best_articles\n",
    "c_sim = process_tfidf_similarity(turn, documents)\n",
    "selected_article_id = c_sim.argmax()\n",
    "selected_article = articles[selected_article_id]\n",
    "print(f\"'{articles[selected_article_id]}' has been selected as the most relevant article\")\n",
    "selected_document = documents[selected_article_id]\n",
    "\"\"\"\n",
    "elif len(best_articles) == 0:\n",
    "    selected_document = best_articles[0]\n",
    "else:\n",
    "    selected_document = None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7607869-9631-40a5-a7af-f6a0e53f1416",
   "metadata": {},
   "source": [
    "## Sentence Similarity using MpNet\n",
    "Worth considering since if we give Topic Modelling the burden of narrowing down the knowledge source, then we can go straight to the Wikipedia article we want\n",
    "HOWEVER, for practical applications, being able to narrow down documents is still useful.. might want to keep TF-IDF in there.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3933b0-777c-4e55-8f46-e4f4e3e9a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "n_gram_range = (5, 5)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "#count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "#candidates = count.get_feature_names()\n",
    "#print(selected_document)\n",
    "message = turn\n",
    "selected_doc_tok_orig = sent_tokenize(selected_document)\n",
    "selected_doc_tok = [selected_article + \" \" + s for s in selected_doc_tok_orig]\n",
    "\n",
    "model = SentenceTransformer('../../models/all-mpnet-base-v2', device='cuda')\n",
    "doc_embedding = model.encode([message])\n",
    "candidate_embeddings = model.encode(selected_doc_tok)\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "print(distances.shape)\n",
    "keywords = [selected_doc_tok[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "idx = distances.argsort()[0][-top_n:]\n",
    "for index in idx:\n",
    "    print(f\"dist: {distances[0][index]}, sent: {selected_doc_tok_orig[index]}, index: {index}\")\n",
    "\n",
    "#print(keywords)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38afe9-9e18-49ea-bc53-660e20e934d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a1460-c2f1-4194-ae9b-dd4f9c0d110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "page = wikipedia.page('Lewis Hamilton')\n",
    "print(len(nltk.tokenize.word_tokenize(page.content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da62815-35e5-4009-b6fa-4478c2019447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
