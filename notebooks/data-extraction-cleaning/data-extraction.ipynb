{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c575757-ba67-4081-bb1f-df64269431fd",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b66bf1-850e-4b47-b76f-42c24cd8f782",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import wikipedia\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49386836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694331d2-8272-4fc9-a44d-b0631ac41fb3",
   "metadata": {},
   "source": [
    "## Extras Nouns from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7301d-7870-4d7c-98fa-ece1e0bc65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"I heard Pfizer works pretty well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757ca8b-a713-40a9-aa99-68e50864005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    wordsList = word_tokenize(sentence)\n",
    "    print(wordsList)\n",
    "    # wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c409f-4c7a-4cda-a7d8-9802a1ed30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sent_tokenize(message)\n",
    "nouns = []\n",
    "for sentence in tokenized:\n",
    "    tagged = tag_sentence(sentence)\n",
    "    print(tagged)\n",
    "    nouns.extend([tag[0] for tag in tagged if tag[1][:2] in ['NN', 'CD']])\n",
    "print(nouns)\n",
    "topic_search_str = ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce5995a-ebd0-4d28-bfd9-49c197684fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lego Star Wars: The Skywalker Saga',\n",
       " 'Skywalker family',\n",
       " 'Star Wars: The Rise of Skywalker',\n",
       " 'Darth Vader']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = wikipedia.search(' '.join(['actor', 'character', 'Skywalker']), results = 4)\n",
    "article = articles[0]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e401314-6559-4392-bd77-117db079b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "# ========== 2. Using urllib & BeatifulSoup ==========\n",
    "# Import packages\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Specify url of the web page\n",
    "page = requests.get(f\"https://en.wikipedia.org/wiki/{article}\")\n",
    "\n",
    "# scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# Extract the plain text content from paragraphs\n",
    "paras = []\n",
    "all_paragraphs = soup.find_all('p', class_=lambda x: x != 'mw-empty-elt')\n",
    "intro_para = \"\"\n",
    "inIntroPara = False\n",
    "\n",
    "for p_id, paragraph in enumerate(all_paragraphs):\n",
    "    p_text = re.sub(r\"\\[.*?\\]+\", '', paragraph.text)\n",
    "    p_tok = nltk.tokenize.sent_tokenize(p_text)\n",
    "    if p_id == 0:\n",
    "        intro_para = p_text\n",
    "        inIntroPara = True\n",
    "    elif len(p_tok) > 1:\n",
    "        paras.extend([' '.join(chunk) for chunk in chunk(p_tok, 8)])\n",
    "        inIntroPara = False\n",
    "\"\"\"\n",
    "elif paragraph.previous_sibling is not None and paragraph.previous_sibling.name == 'p':\n",
    "    if inIntroPara:\n",
    "        intro_para = f\"{intro_para} {str(p_text)}\"\n",
    "    else:\n",
    "        paras[-1] = f\"{paras[-1]} {str(p_text)}\"\n",
    "\"\"\"\n",
    "\n",
    "# Extract text from paragraph headers\n",
    "heads = []\n",
    "for head in soup.find_all('span', attrs={'mw-headline'}):\n",
    "    heads.append(str(head.text))\n",
    "\n",
    "# The first paragraph is the introductory paragraph and doesn't have a heading\n",
    "# Set its heading as the document title\n",
    "heads.insert(0, article)\n",
    "paras.insert(0, intro_para)\n",
    "\n",
    "# Drop footnote superscripts in brackets\n",
    "#text = \n",
    "\n",
    "# Replace '\\n' (a new line) with '' and end the string at $1000.\n",
    "#text = text.replace('\\n', '')[:-11]\n",
    "#print(text)\n",
    "\"\"\"\n",
    "for i in range(len(paras)):\n",
    "    if len(nltk.tokenize.sent_tokenize(paras[i])) > 1:\n",
    "        print(paras[i], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b67c8-9542-4bcb-a019-b0917d13676e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15a306a",
   "metadata": {},
   "source": [
    "# Find paragraph similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e406d04-60d9-4dbd-bf03-25f5853b776d",
   "metadata": {},
   "source": [
    "## Finding paragraph similarity using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"How long does it take for symptoms to appear?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f196e-57b0-47bc-9d7e-e0fa37dc690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "\n",
    "def process_tfidf_similarity(base_document, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # To make uniformed vectors, both documents need to be combined first.\n",
    "    d = [base_document]\n",
    "    d.extend(documents)\n",
    "    embeddings = vectorizer.fit_transform(documents)\n",
    "    embeddings = vstack((vectorizer.transform([base_document]), embeddings))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', binary=True, ngram_range=(1,3), analyzer='char_wb')\n",
    "    eds = vectorizer.fit_transform(d)\n",
    "    print(type(embeddings))\n",
    "    print(embeddings.shape, len(d))\n",
    "\n",
    "    cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    \n",
    "    cosine_similarities = cosine_similarity(eds[0:1], eds[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93baf7-25aa-453e-8f83-a27d09222260",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = paras\n",
    "t1 = time.time()\n",
    "c_sim = process_tfidf_similarity(message, documents)\n",
    "selected_article_id = c_sim.argmax()\n",
    "print(f\"TIME: {time.time() - t1}\")\n",
    "#selected_article = heads[selected_article_id]\n",
    "#print(f\"'{heads[selected_article_id]}' has been selected as the most relevant article\")\n",
    "selected_document = documents[selected_article_id]\n",
    "print(selected_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7b5b5-abf7-4bd9-a53e-e9003f3c7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heads)\n",
    "print(paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100f2e0-f33f-48b6-a014-76ae3b5126ba",
   "metadata": {},
   "source": [
    "# Paragraph similarity using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for para in paras:\n",
    "    sents.extend(nltk.tokenize.sent_tokenize(para))\n",
    "#print('\\n'.join(sents))\n",
    "print(len(sents), len(paras))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37eccb-b276-4217-9046-7e7c99ad75bb",
   "metadata": {},
   "source": [
    "## Using Sent Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad56c3",
   "metadata": {},
   "source": [
    "### Using MpNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a9995-b613-4a5c-bb02-e4e46865fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('../../models/all-mpnet-base-v2', device='cuda')\n",
    "t1 = time.time()\n",
    "doc_embedding = model.encode([message])\n",
    "candidate_embeddings = model.encode(paras)\n",
    "top_n = 2\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings).flatten()\n",
    "keywords = [paras[index] for index in ((-distances).argsort())[:top_n]]\n",
    "print(time.time() - t1)\n",
    "print(paras[distances.argmax()])\n",
    "print('\\n'.join(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ae0d0",
   "metadata": {},
   "source": [
    "### Using all-MiniLM-L6-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('../../models/all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1152cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "#m1 = [\"I love cricket!\", \"I don't know if the vaccines are effective\", \"I love riding horses!\", \"Do you watch football?\"]\n",
    "#m2 = \"Did you watch Australia vs Pakistan?\"\n",
    "m1 = ['match', 'football']\n",
    "m2 = \"game\"\n",
    "doc_embedding = model.encode([m2])\n",
    "candidate_embeddings = model.encode(m1)\n",
    "top_n = 2\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings).flatten()\n",
    "print(time.time() - t1)\n",
    "print(distances)\n",
    "\n",
    "\"\"\"\n",
    "keywords = [paras[index] for index in ((-distances).argsort())[:top_n]]\n",
    "print(time.time() - t1)\n",
    "print(paras[distances.argmax()])\n",
    "print('\\n'.join(keywords))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123baf2",
   "metadata": {},
   "source": [
    "### Clustering cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#y = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]\n",
    "t1 = time.time()\n",
    "y = sorted([0.12140948, 0.426371, 0.11862079, 0.44534147, 0.17006755, 0.55, 0.00, 0.00, 0.00, 0.00])\n",
    "#y = [x*30 for x in y]\n",
    "print(y)\n",
    "x = range(len(y))\n",
    "m = np.matrix([x, y]).transpose()\n",
    "\n",
    "from scipy.cluster.vq import kmeans\n",
    "kclust = kmeans(m, 3)\n",
    "\n",
    "cluster_indices = kclust[0][:, 0]\n",
    "assigned_clusters = [abs(cluster_indices - e).argmin() for e in x]\n",
    "print(f\"time elapsed: {time.time() - t1}\")\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc639b7",
   "metadata": {},
   "source": [
    "### Clustering cosine similarities: Proper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "def find_highest_similarity_scores(scores, n=3):\n",
    "    s_idxs = sorted(range(len(scores)), key=scores.__getitem__)\n",
    "    s = [scores[i] for i in s_idxs]\n",
    "    s_len = len(s)\n",
    "    s_range = range(s_len)\n",
    "    \n",
    "    n = min(n, s_len)\n",
    "    kclust = kmeans(np.matrix([s_range, s]).transpose(), n)\n",
    "    assigned_clusters = [abs(kclust[0][:, 0] - e).argmin() for e in s_range]\n",
    "    \n",
    "    print(assigned_clusters)\n",
    "    \n",
    "    highest_cluster = assigned_clusters[-1]\n",
    "    highest_idxs = []\n",
    "    for i in range(s_len-1, -1, -1):\n",
    "        if assigned_clusters[i] != highest_cluster:\n",
    "            return highest_idxs\n",
    "        highest_idxs.append(s_idxs[i])\n",
    "    return highest_idxs\n",
    "\n",
    "#t = [0.12140948, 0.426371, 0.11862079, 0.44534147, 0.17006755, 0.55, 0.00, 0.00, 0.00, 0.00]\n",
    "t = [0, 0.25, 0.75, 1]\n",
    "[t[i] for i in find_highest_similarity_scores(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718513bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "#x = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]\n",
    "t1 = time.time()\n",
    "x = [e*30 for e in y]\n",
    "X = np.array(list(zip(x,np.zeros(len(x)))), dtype=np.int)\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.6)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "print(f\"time elapsed: {time.time() - t1}\")\n",
    "\n",
    "for k in range(n_clusters_):\n",
    "    my_members = labels == k\n",
    "    print (\"cluster {0}: {1}\".format(k, X[my_members]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a17368-ddb5-4342-9cc6-eabac35e6e4e",
   "metadata": {},
   "source": [
    "## Using BERT with averaging sent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073ccda-7ac1-499c-843a-20bc55b4c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# This will download and load the pretrained model offered by UKPLab.\n",
    "model = SentenceTransformer('../../models/bert-base-cased-squad2', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_embeddings(docs):\n",
    "    docs_embeddings = None\n",
    "    for doc in docs:\n",
    "        sentences = sent_tokenize(doc)\n",
    "        base_embeddings_sentences = model.encode(sentences)\n",
    "        base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)\n",
    "        print(base_embeddings.shape)\n",
    "        if docs_embeddings is None:\n",
    "            docs_embeddings = base_embeddings\n",
    "        else:\n",
    "            print(docs_embeddings.shape, base_embeddings.shape)\n",
    "            docs_embeddings = np.row_stack((docs_embeddings, base_embeddings))\n",
    "    return docs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa482b49-06dd-425d-8986-a9cf181c0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = \"I love football\"\n",
    "# x = \"I like football\"\n",
    "# y = \"Computer science involves cryptography\"\n",
    "# message = \"Which team beat New Zealand in the 1992 World Cup?\"\n",
    "t1 = time.time()\n",
    "docs_embds = calculate_bert_embeddings(m1)\n",
    "msg_embds = calculate_bert_embeddings([m2]).reshape(1,-1)\n",
    "print(msg_embds.shape, docs_embds.shape)\n",
    "c_sim = cosine_similarity(msg_embds, docs_embds).flatten()\n",
    "print(f\"TIME: {time.time() - t1}\")\n",
    "print(c_sim)\n",
    "paras[c_sim.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e02902-9015-4228-9506-b4c4da20cd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fab4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "page = wikipedia.page('COVID-19')\n",
    "print(len(nltk.tokenize.word_tokenize(page.content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad7603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
