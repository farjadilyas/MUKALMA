{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c575757-ba67-4081-bb1f-df64269431fd",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08b66bf1-850e-4b47-b76f-42c24cd8f782",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import wikipedia\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694331d2-8272-4fc9-a44d-b0631ac41fb3",
   "metadata": {},
   "source": [
    "## Extras Nouns from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c7301d-7870-4d7c-98fa-ece1e0bc65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"I heard Pfizer works pretty well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0757ca8b-a713-40a9-aa99-68e50864005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    wordsList = word_tokenize(sentence)\n",
    "    print(wordsList)\n",
    "    # wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2c409f-4c7a-4cda-a7d8-9802a1ed30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'heard', 'Pfizer', 'works', 'pretty', 'well']\n",
      "[('I', 'PRP'), ('heard', 'VBP'), ('Pfizer', 'NNP'), ('works', 'NNS'), ('pretty', 'RB'), ('well', 'RB')]\n",
      "['Pfizer', 'works']\n"
     ]
    }
   ],
   "source": [
    "tokenized = sent_tokenize(message)\n",
    "nouns = []\n",
    "for sentence in tokenized:\n",
    "    tagged = tag_sentence(sentence)\n",
    "    print(tagged)\n",
    "    nouns.extend([tag[0] for tag in tagged if tag[1][:2] in ['NN', 'CD']])\n",
    "print(nouns)\n",
    "topic_search_str = ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ce5995a-ebd0-4d28-bfd9-49c197684fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010â€“11 FC Barcelona season',\n",
       " 'Marc Overmars',\n",
       " 'Captain Tsubasa',\n",
       " 'Rose Bowl (stadium)']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = wikipedia.search(\"game Barcelona match football yesterday day\", results = 4)\n",
    "article = articles[0]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e401314-6559-4392-bd77-117db079b48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(paras)):\\n    if len(nltk.tokenize.sent_tokenize(paras[i])) > 1:\\n        print(paras[i], \"\\n\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "# ========== 2. Using urllib & BeatifulSoup ==========\n",
    "# Import packages\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Specify url of the web page\n",
    "page = requests.get(f\"https://en.wikipedia.org/wiki/{article}\")\n",
    "\n",
    "# scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# Extract the plain text content from paragraphs\n",
    "paras = []\n",
    "all_paragraphs = soup.find_all('p', class_=lambda x: x != 'mw-empty-elt')\n",
    "intro_para = \"\"\n",
    "inIntroPara = False\n",
    "\n",
    "for p_id, paragraph in enumerate(all_paragraphs):\n",
    "    p_text = re.sub(r\"\\[.*?\\]+\", '', paragraph.text)\n",
    "    p_tok = nltk.tokenize.sent_tokenize(p_text)\n",
    "    if p_id == 0:\n",
    "        intro_para = p_text\n",
    "        inIntroPara = True\n",
    "    elif len(p_tok) > 1:\n",
    "        paras.extend([' '.join(chunk) for chunk in chunk(p_tok, 8)])\n",
    "        inIntroPara = False\n",
    "\"\"\"\n",
    "elif paragraph.previous_sibling is not None and paragraph.previous_sibling.name == 'p':\n",
    "    if inIntroPara:\n",
    "        intro_para = f\"{intro_para} {str(p_text)}\"\n",
    "    else:\n",
    "        paras[-1] = f\"{paras[-1]} {str(p_text)}\"\n",
    "\"\"\"\n",
    "\n",
    "# Extract text from paragraph headers\n",
    "heads = []\n",
    "for head in soup.find_all('span', attrs={'mw-headline'}):\n",
    "    heads.append(str(head.text))\n",
    "\n",
    "# The first paragraph is the introductory paragraph and doesn't have a heading\n",
    "# Set its heading as the document title\n",
    "heads.insert(0, article)\n",
    "paras.insert(0, intro_para)\n",
    "\n",
    "# Drop footnote superscripts in brackets\n",
    "#text = \n",
    "\n",
    "# Replace '\\n' (a new line) with '' and end the string at $1000.\n",
    "#text = text.replace('\\n', '')[:-11]\n",
    "#print(text)\n",
    "\"\"\"\n",
    "for i in range(len(paras)):\n",
    "    if len(nltk.tokenize.sent_tokenize(paras[i])) > 1:\n",
    "        print(paras[i], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b67c8-9542-4bcb-a019-b0917d13676e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15a306a",
   "metadata": {},
   "source": [
    "# Find paragraph similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e406d04-60d9-4dbd-bf03-25f5853b776d",
   "metadata": {},
   "source": [
    "## Finding paragraph similarity using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd05cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"How long does it take for symptoms to appear?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "506f196e-57b0-47bc-9d7e-e0fa37dc690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "\n",
    "def process_tfidf_similarity(base_document, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # To make uniformed vectors, both documents need to be combined first.\n",
    "    d = [base_document]\n",
    "    d.extend(documents)\n",
    "    embeddings = vectorizer.fit_transform(documents)\n",
    "    embeddings = vstack((vectorizer.transform([base_document]), embeddings))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', binary=True, ngram_range=(1,3), analyzer='char_wb')\n",
    "    eds = vectorizer.fit_transform(d)\n",
    "    print(type(embeddings))\n",
    "    print(embeddings.shape, len(d))\n",
    "\n",
    "    cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    \n",
    "    cosine_similarities = cosine_similarity(eds[0:1], eds[1:]).flatten()\n",
    "    print(cosine_similarities)\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c93baf7-25aa-453e-8f83-a27d09222260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(117, 2350) 117\n",
      "[0.         0.00595723 0.17353474 0.02860639 0.00385764 0.02271634\n",
      " 0.03491407 0.07466478 0.10383455 0.05496484 0.         0.04936966\n",
      " 0.10576509 0.02387282 0.02750236 0.01230172 0.00443719 0.01919357\n",
      " 0.02881878 0.02218652 0.02740216 0.02918774 0.01569957 0.02268919\n",
      " 0.00427336 0.02746613 0.03354062 0.00849238 0.02973214 0.02936456\n",
      " 0.02349747 0.11530548 0.01210011 0.02067262 0.02744454 0.01983409\n",
      " 0.02043159 0.05381977 0.0400098  0.03828553 0.00693764 0.01174831\n",
      " 0.02354131 0.01464075 0.00419928 0.00845099 0.00494211 0.03807895\n",
      " 0.00297784 0.0278317  0.03688372 0.00387621 0.0084644  0.00447949\n",
      " 0.03369848 0.         0.0236053  0.0141225  0.05253197 0.03342193\n",
      " 0.03572339 0.13659602 0.10684404 0.01981251 0.02236727 0.02584363\n",
      " 0.02014085 0.0213917  0.         0.04091285 0.10466337 0.08619873\n",
      " 0.00564629 0.04899473 0.10403157 0.         0.00524576 0.02128446\n",
      " 0.00438791 0.00766479 0.01271297 0.         0.03436535 0.01120181\n",
      " 0.02700251 0.01863778 0.02453803 0.00529207 0.03985568 0.03470524\n",
      " 0.05265187 0.04109949 0.01937177 0.03152321 0.         0.00629148\n",
      " 0.048001   0.18742377 0.         0.01569545 0.02920217 0.\n",
      " 0.01136778 0.         0.00411902 0.         0.03515219 0.01724118\n",
      " 0.         0.03997516 0.05083543 0.00523217 0.03935941 0.0276095\n",
      " 0.02936998 0.00688639]\n",
      "[0.00625369 0.07153783 0.10772414 0.13559445 0.06255693 0.11498516\n",
      " 0.05937515 0.11721887 0.11491237 0.18812116 0.10683234 0.13715073\n",
      " 0.10376896 0.06932546 0.05768964 0.05671427 0.07570966 0.09790366\n",
      " 0.10312124 0.07708852 0.11349606 0.08743862 0.10234686 0.07853478\n",
      " 0.07768894 0.1007169  0.08155342 0.08293358 0.12339875 0.06141082\n",
      " 0.06854083 0.14558701 0.0600938  0.08206575 0.11189884 0.08092326\n",
      " 0.11393603 0.12628542 0.07450317 0.09546301 0.09862958 0.07282412\n",
      " 0.09492574 0.10054594 0.07245571 0.06085297 0.06971026 0.06274361\n",
      " 0.09367513 0.09427038 0.07978253 0.09856872 0.10859683 0.06472149\n",
      " 0.06738412 0.04912934 0.09535936 0.11159426 0.12310355 0.15425729\n",
      " 0.09376777 0.16813409 0.09238949 0.07574586 0.07490284 0.09752027\n",
      " 0.09088774 0.11564398 0.08601589 0.12067025 0.13767647 0.1360203\n",
      " 0.10018874 0.14020293 0.18877467 0.08854112 0.06954143 0.08939208\n",
      " 0.04094919 0.09884493 0.0645461  0.06897952 0.06916018 0.10467177\n",
      " 0.07932836 0.06655059 0.06065352 0.04792564 0.09942051 0.10457599\n",
      " 0.07292447 0.10227436 0.05706128 0.1295293  0.10425323 0.07605023\n",
      " 0.1161231  0.21179691 0.08130537 0.11121449 0.06159931 0.14350747\n",
      " 0.08430718 0.09125602 0.0579212  0.06351345 0.08715008 0.10274894\n",
      " 0.12883669 0.07617373 0.06591153 0.05964734 0.0816179  0.09563997\n",
      " 0.08953001 0.06071661]\n",
      "TIME: 0.1426856517791748\n",
      "The virus does not appear to be able to infect pigs, ducks, or chickens at all. Mice, rats, and rabbits, if they can be infected at all, are unlikely to be involved in spreading the virus.\n"
     ]
    }
   ],
   "source": [
    "documents = paras\n",
    "t1 = time.time()\n",
    "c_sim = process_tfidf_similarity(message, documents)\n",
    "selected_article_id = c_sim.argmax()\n",
    "print(f\"TIME: {time.time() - t1}\")\n",
    "#selected_article = heads[selected_article_id]\n",
    "#print(f\"'{heads[selected_article_id]}' has been selected as the most relevant article\")\n",
    "selected_document = documents[selected_article_id]\n",
    "print(selected_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7b5b5-abf7-4bd9-a53e-e9003f3c7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heads)\n",
    "print(paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100f2e0-f33f-48b6-a014-76ae3b5126ba",
   "metadata": {},
   "source": [
    "# Paragraph similarity using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ccd8e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farja\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35a40d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 116\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for para in paras:\n",
    "    sents.extend(nltk.tokenize.sent_tokenize(para))\n",
    "#print('\\n'.join(sents))\n",
    "print(len(sents), len(paras))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37eccb-b276-4217-9046-7e7c99ad75bb",
   "metadata": {},
   "source": [
    "## Using Sent Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad56c3",
   "metadata": {},
   "source": [
    "### Using MpNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4a9995-b613-4a5c-bb02-e4e46865fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.693247318267822\n",
      "As is common with infections, there is a delay between the moment a person first becomes infected and the appearance of the first symptoms. The median delay for COVID-19 is four to five days possibly being infectious on 1-4 of those days. Most symptomatic people experience symptoms within two to seven days after exposure, and almost all will experience at least one symptom within 12 days.\n",
      "As is common with infections, there is a delay between the moment a person first becomes infected and the appearance of the first symptoms. The median delay for COVID-19 is four to five days possibly being infectious on 1-4 of those days. Most symptomatic people experience symptoms within two to seven days after exposure, and almost all will experience at least one symptom within 12 days.\n",
      "Some early studies suggest that 10â€“20% of people with COVIDâ€‘19 will experience symptoms lasting longer than a month. A majority of those who were admitted to hospital with severe disease report long-term problems, including fatigue and shortness of breath. About 5â€“10% of patients admitted to hospital progress to severe or critical disease, including pneumonia and acute respiratory failure.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('../../models/all-mpnet-base-v2', device='cuda')\n",
    "t1 = time.time()\n",
    "doc_embedding = model.encode([message])\n",
    "candidate_embeddings = model.encode(paras)\n",
    "top_n = 2\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings).flatten()\n",
    "keywords = [paras[index] for index in ((-distances).argsort())[:top_n]]\n",
    "print(time.time() - t1)\n",
    "print(paras[distances.argmax()])\n",
    "print('\\n'.join(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ae0d0",
   "metadata": {},
   "source": [
    "### Using all-MiniLM-L6-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8feecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('../../models/all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1152cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34549570083618164\n",
      "[0.55318475 0.54193956]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nkeywords = [paras[index] for index in ((-distances).argsort())[:top_n]]\\nprint(time.time() - t1)\\nprint(paras[distances.argmax()])\\nprint('\\n'.join(keywords))\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "#m1 = [\"I love cricket!\", \"I don't know if the vaccines are effective\", \"I love riding horses!\", \"Do you watch football?\"]\n",
    "#m2 = \"Did you watch Australia vs Pakistan?\"\n",
    "m1 = ['match', 'football']\n",
    "m2 = \"game\"\n",
    "doc_embedding = model.encode([m2])\n",
    "candidate_embeddings = model.encode(m1)\n",
    "top_n = 2\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings).flatten()\n",
    "print(time.time() - t1)\n",
    "print(distances)\n",
    "\n",
    "\"\"\"\n",
    "keywords = [paras[index] for index in ((-distances).argsort())[:top_n]]\n",
    "print(time.time() - t1)\n",
    "print(paras[distances.argmax()])\n",
    "print('\\n'.join(keywords))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123baf2",
   "metadata": {},
   "source": [
    "### Clustering cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2d3f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.11862079, 0.12140948, 0.17006755, 0.426371, 0.44534147, 0.55]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatrix([x, y])\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kmeans\n\u001b[1;32m---> 13\u001b[0m kclust \u001b[38;5;241m=\u001b[39m kmeans(m, \u001b[43mn\u001b[49m)\n\u001b[0;32m     15\u001b[0m cluster_indices \u001b[38;5;241m=\u001b[39m kclust[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m assigned_clusters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mabs\u001b[39m(cluster_indices \u001b[38;5;241m-\u001b[39m e)\u001b[38;5;241m.\u001b[39margmin() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m x]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#y = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]\n",
    "t1 = time.time()\n",
    "y = sorted([0.12140948, 0.426371, 0.11862079, 0.44534147, 0.17006755, 0.55, 0.00, 0.00, 0.00, 0.00])\n",
    "#y = [x*30 for x in y]\n",
    "print(y)\n",
    "x = range(len(y))\n",
    "m = np.matrix([x, y]).transpose()\n",
    "\n",
    "from scipy.cluster.vq import kmeans\n",
    "kclust = kmeans(m, 3)\n",
    "\n",
    "cluster_indices = kclust[0][:, 0]\n",
    "assigned_clusters = [abs(cluster_indices - e).argmin() for e in x]\n",
    "print(f\"time elapsed: {time.time() - t1}\")\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc639b7",
   "metadata": {},
   "source": [
    "### Clustering cosine similarities: Proper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9022d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0.75]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "def find_highest_similarity_scores(scores, n=3):\n",
    "    s_idxs = sorted(range(len(scores)), key=scores.__getitem__)\n",
    "    s = [scores[i] for i in s_idxs]\n",
    "    s_len = len(s)\n",
    "    s_range = range(s_len)\n",
    "    \n",
    "    n = min(n, s_len)\n",
    "    kclust = kmeans(np.matrix([s_range, s]).transpose(), n)\n",
    "    assigned_clusters = [abs(kclust[0][:, 0] - e).argmin() for e in s_range]\n",
    "    \n",
    "    print(assigned_clusters)\n",
    "    \n",
    "    highest_cluster = assigned_clusters[-1]\n",
    "    highest_idxs = []\n",
    "    for i in range(s_len-1, -1, -1):\n",
    "        if assigned_clusters[i] != highest_cluster:\n",
    "            return highest_idxs\n",
    "        highest_idxs.append(s_idxs[i])\n",
    "    return highest_idxs\n",
    "\n",
    "#t = [0.12140948, 0.426371, 0.11862079, 0.44534147, 0.17006755, 0.55, 0.00, 0.00, 0.00, 0.00]\n",
    "t = [0, 0.25, 0.75, 1]\n",
    "[t[i] for i in find_highest_similarity_scores(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "718513bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.008007287979125977\n",
      "cluster 0: [[12  0]\n",
      " [13  0]\n",
      " [16  0]]\n",
      "cluster 1: [[3 0]\n",
      " [3 0]\n",
      " [5 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farja\\AppData\\Local\\Temp\\ipykernel_14976\\4022284999.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.array(list(zip(x,np.zeros(len(x)))), dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "#x = [1,1,5,6,1,5,10,22,23,23,50,51,51,52,100,112,130,500,512,600,12000,12230]\n",
    "t1 = time.time()\n",
    "x = [e*30 for e in y]\n",
    "X = np.array(list(zip(x,np.zeros(len(x)))), dtype=np.int)\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.6)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "print(f\"time elapsed: {time.time() - t1}\")\n",
    "\n",
    "for k in range(n_clusters_):\n",
    "    my_members = labels == k\n",
    "    print (\"cluster {0}: {1}\".format(k, X[my_members]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a17368-ddb5-4342-9cc6-eabac35e6e4e",
   "metadata": {},
   "source": [
    "## Using BERT with averaging sent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2073ccda-7ac1-499c-843a-20bc55b4c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name ../../models/bert-base-cased-squad2. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at ../../models/bert-base-cased-squad2 were not used when initializing BertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# This will download and load the pretrained model offered by UKPLab.\n",
    "model = SentenceTransformer('../../models/bert-base-cased-squad2', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0924f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_embeddings(docs):\n",
    "    docs_embeddings = None\n",
    "    for doc in docs:\n",
    "        sentences = sent_tokenize(doc)\n",
    "        base_embeddings_sentences = model.encode(sentences)\n",
    "        base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)\n",
    "        print(base_embeddings.shape)\n",
    "        if docs_embeddings is None:\n",
    "            docs_embeddings = base_embeddings\n",
    "        else:\n",
    "            print(docs_embeddings.shape, base_embeddings.shape)\n",
    "            docs_embeddings = np.row_stack((docs_embeddings, base_embeddings))\n",
    "    return docs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa482b49-06dd-425d-8986-a9cf181c0373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(768,)\n",
      "(768,) (768,)\n",
      "(768,)\n",
      "(2, 768) (768,)\n",
      "(768,)\n",
      "(3, 768) (768,)\n",
      "(768,)\n",
      "(4, 768) (768,)\n",
      "(768,)\n",
      "(5, 768) (768,)\n",
      "(768,)\n",
      "(6, 768) (768,)\n",
      "(768,)\n",
      "(7, 768) (768,)\n",
      "(768,)\n",
      "(8, 768) (768,)\n",
      "(768,)\n",
      "(9, 768) (768,)\n",
      "(768,)\n",
      "(10, 768) (768,)\n",
      "(768,)\n",
      "(11, 768) (768,)\n",
      "(768,)\n",
      "(12, 768) (768,)\n",
      "(768,)\n",
      "(13, 768) (768,)\n",
      "(768,)\n",
      "(14, 768) (768,)\n",
      "(768,)\n",
      "(1, 768) (15, 768)\n",
      "TIME: 3.9620213508605957\n",
      "[0.763141   0.7383532  0.7377845  0.7588626  0.76193196 0.75090337\n",
      " 0.77208793 0.71912754 0.71682876 0.78318155 0.74859285 0.76855487\n",
      " 0.7186981  0.73935056 0.71325564]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Co-hosts New Zealand proved the surprise package of the tournament, winning their first seven consecutive games to finish on top of the table after the round-robin. The other hosts, Australia, one of the pre-tournament favourites lost their first two matches. They recovered somewhat to win four of the remaining six, but narrowly missed out on the semi-finals. The West Indies also finished with a 4â€“4 record, but were just behind Australia on run-rate. South Africa made a triumphant return to international cricket with a win over Australia at the Sydney Cricket Ground in their first match. They and England had solid campaigns and easily qualified for the semis, despite upset losses to Sri Lanka and Zimbabwe  respectively. India had a disappointing tournament and never looked likely to progress beyond the round-robin. Sri Lanka were still establishing themselves at the highest level and beat only Zimbabwe (who did not yet have Test status) and South Africa.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m = \"I love football\"\n",
    "# x = \"I like football\"\n",
    "# y = \"Computer science involves cryptography\"\n",
    "# message = \"Which team beat New Zealand in the 1992 World Cup?\"\n",
    "t1 = time.time()\n",
    "docs_embds = calculate_bert_embeddings(m1)\n",
    "msg_embds = calculate_bert_embeddings([m2]).reshape(1,-1)\n",
    "print(msg_embds.shape, docs_embds.shape)\n",
    "c_sim = cosine_similarity(msg_embds, docs_embds).flatten()\n",
    "print(f\"TIME: {time.time() - t1}\")\n",
    "print(c_sim)\n",
    "paras[c_sim.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e02902-9015-4228-9506-b4c4da20cd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03fab4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12293\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "page = wikipedia.page('COVID-19')\n",
    "print(len(nltk.tokenize.word_tokenize(page.content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad7603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
